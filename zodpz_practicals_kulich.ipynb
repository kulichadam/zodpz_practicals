{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to classification in python  \n",
    "\n",
    "Developped for Remote sensing image processing course  \n",
    "Department of Applied Geoinformatics and Carthography, Faculty of Science, Charles University  \n",
    "Author: Adam Kulich, 2025  \n",
    "\n",
    "library requirements: gdal, numpy, matplotlib, scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from osgeo import gdal, ogr\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opening and visualization of the image  \n",
    "\n",
    "For any kind of image analysis, it needs to be converted into a multidimensional matrix containing the DN values of the individual pixels in each band. For this step, we use the gdal library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open image using gdal\n",
    "image_path = \"INSERT_IMAGE_PATH.tif\"\n",
    "image = gdal.Open(image_path)\n",
    "\n",
    "#you can check image properties\n",
    "n_col = image.RasterXSize\n",
    "n_row = image.RasterYSize\n",
    "n_bands = image.RasterCount\n",
    "print(f\"Width: {n_col}, Height: {n_row}, Bands: {n_bands}\")\n",
    "\n",
    "#or georeferencing information\n",
    "projection = image.GetProjection() \n",
    "geotransform = image.GetGeoTransform()\n",
    "print(f\"Projection: {projection}\") \n",
    "print(f\"GeoTransform: {geotransform}\")\n",
    "\n",
    "#see more about GeoTransform attribute at https://gdal.org/en/stable/tutorials/geotransforms_tut.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we opened the image and opened its properties, we can rewrite it into an array of 8-bit integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the bands as arrays\n",
    "red = image.GetRasterBand(1).ReadAsArray().astype(np.uint8) \n",
    "green = image.GetRasterBand(2).ReadAsArray().astype(np.uint8) \n",
    "blue = image.GetRasterBand(3).ReadAsArray().astype(np.uint8)\n",
    "\n",
    "#next it is needed to stack it in one array\n",
    "rgb_array = np.dstack((red, green, blue))\n",
    "#let's check the value at a specific place\n",
    "print(\"Value of red band at 120th row and 120th column: {rgb_array[120,120,0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can display the image with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rgb_array)\n",
    "plt.title(\"RGB Composite\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting data from the image for training, validation and testing  \n",
    "To open vectors, we use org, which is part of gdal library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the vector with ogr\n",
    "vector_path = \"INSERT_VECTOR_PATH\"\n",
    "vector = ogr.Open(vector_path)\n",
    "#extracting the feature layer from the file\n",
    "vector_layer = vector.GetLayer()\n",
    "\n",
    "#creating an empty raster (of the same shape as the image)\n",
    "out_raster_ds = gdal.GetDriverByName(\"MEM\").Create(\"\", n_col, n_row, 1, gdal.GDT_Byte)\n",
    "\n",
    "#filling the empty raster with vector mask (0 or classvalue)\n",
    "status = gdal.RasterizeLayer(out_raster_ds, [1], vector.GetLayerByIndex(0), None, None, [0], ['ALL_TOUCHED=TRUE', f'ATTRIBUTE={classvalue}'])\n",
    "\n",
    "#read the raster mask as an array\n",
    "rasterized_vector = out_raster_ds.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "#extract input data from the image and mask\n",
    "X = rgb_array[rasterized_vector > 0, :]\n",
    "#extract target data from the rasterized vector\n",
    "y = rasterized_vector[rasterized_vector > 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First look at scikit-learn library\n",
    "Let's explore the possibilities in the scikit-learn library: https://scikit-learn.org/stable/  \n",
    "\n",
    "First, we need to separate train and test data, so we can evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_target, test_target = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do the same if the ground truth dataset is defined by polygons, not points?  \n",
    "\n",
    "Now let's try to create a random forest model and evaulate it using test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is needed to import parts of the sklearn library\n",
    "import sklearn.ensemble\n",
    "import sklearn.preprocessing\n",
    "\n",
    "#for most data it is worth it to scale it between 0 and 1\n",
    "scaler =  sklearn.preprocessing.MinMaxScaler()\n",
    "\n",
    "#fitting the scaler and transforming the train input\n",
    "scaler.fit_transform(train_data)\n",
    "#transforming the test input\n",
    "scaler.transform(test_data)\n",
    "\n",
    "#initialize the random forest classifier\n",
    "model = sklearn.ensemble.RandomForestClassifier()\n",
    "\n",
    "#train the classifier on the train data\n",
    "model.fit(train_data,test_data)\n",
    "\n",
    "#predict the test data\n",
    "prediction = model.predict(test_data)\n",
    "\n",
    "#compute evaluation metrics and print the results\n",
    "accuracy = sklearn.metrics.accuracy_score(test_target, prediction)\n",
    "matrix = sklearn.metrics.confusion_matrix(test_target, prediction)\n",
    "report = sklearn.metrics.classification_report(test_target, prediction, output_dict=True)\n",
    "print(\"Final accuracy is\", accuracy, \"and F-1 score\", report['weighted avg']['f1-score'])\n",
    "print(\"Confusion matrix:\", matrix)\n",
    "print(\"Full report:\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing hyperparameters  \n",
    "Next we can fine ture the model and improve the result a bit.  \n",
    "First, let's show how we can set a hyperparameter. Using the scikit-learn documentation (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier), we can look at parameters of the RandomForestClassifier method. Lets change some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the random forest classifier with ntree=85, mtry=3 and maximum depth=2\n",
    "model2 = sklearn.ensemble.RandomForestClassifier(n_estimators=150,max_features=3,max_depth=2)\n",
    "model2.fit(train_data,test_data)\n",
    "prediction2 = model2.predict(test_data)\n",
    "\n",
    "accuracy2 = sklearn.metrics.accuracy_score(test_target, prediction2)\n",
    "report2 = sklearn.metrics.classification_report(test_target, prediction2, output_dict=True)\n",
    "print(\"Final accuracy is\", accuracy2, \"and F-1 score\", report2['weighted avg']['f1-score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is probably worse than before - how can we find a better solution?\n",
    "\n",
    "One option is grid search using cross-validation. This way we can compare different setting without touching the test set. Comparing the results of multiple evaluations on the test set could lead to overfitting. Scikit-learn library offers a built-in function for grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "#let's first define the parameters we need to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 400],\n",
    "    'max_features': [2, 3],\n",
    "    'max_depth': [3, 5, 7, None],\n",
    "    'min_samples_split': [2, 3, 6],\n",
    "    'min_samples_leaf': [1, 3, 5]\n",
    "}\n",
    "\n",
    "#initialize Grid search\n",
    "grid_search = sklearn.model_selection.GridSearchCV(\n",
    "    estimator=sklearn.ensemble.RandomForestClassifier(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#fit the model\n",
    "grid_search.fit(train_data, train_target)\n",
    "\n",
    "#best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware that the gridsearch might take very long if the grid is large and multidimensional. The example you just ran has to train and evaluate 864 models. This could take very long time if the train dataset is larger and there are more features. As a reasonable alternative, you can use sklearn.model_selection.RandomizedSearchCV().  \n",
    "\n",
    "#### Final image classification\n",
    "\n",
    "Now we can train the final model with the best parameters and classify the whole image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and evaluate the final model\n",
    "final_model = sklearn.ensemble.RandomForestClassifier(grid_search.best_params_)\n",
    "final_model.fit(train_data,test_data)\n",
    "final_prediction = final_model.predict(test_data)\n",
    "\n",
    "final_accuracy = sklearn.metrics.accuracy_score(test_target, final_prediction)\n",
    "final_matrix = sklearn.metrics.confusion_matrix(test_target, final_prediction)\n",
    "final_report = sklearn.metrics.classification_report(test_target, final_prediction, output_dict=True)\n",
    "print(\"Final accuracy is\", final_accuracy, \"and F-1 score\", final_report['weighted avg']['f1-score'])\n",
    "print(\"Confusion matrix:\", final_matrix)\n",
    "print(\"Full report:\", final_report)\n",
    "\n",
    "#reshape the image into (n_samples, n_features) so it can be used as an imput to the model\n",
    "rgb_array = rgb_array.reshape(-1, n_bands)\n",
    "scaler.transform(rgb_array)\n",
    "\n",
    "#predict whole array\n",
    "classification = model.predict(rgb_array)\n",
    "classification = classification.reshape(n_row, n_col)\n",
    "\n",
    "#vizualize the classification\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "#set color map\n",
    "colors = [\"#00B050\", \"#92D050\", \"#00B0F0\", \"#FF0000\"]\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "#create figure and display\n",
    "plt.imshow(classification, cmap=cmap)\n",
    "#add legend\n",
    "cbar = plt.colorbar(ticks=[1, 2, 3, 4])\n",
    "cbar.ax.set_yticklabels([\"Forest\", \"Open grassland and crops\", \"Water body\", \"Urban area\"])\n",
    "\n",
    "plt.title(\"Classified Image\")\n",
    "plt.axis(\"off\")  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also very useful to save the classified image to a new .tif file. This can be done using gdal again, with the information about the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set output path\n",
    "output_path = \"SET_OUTPUT_PATH\"\n",
    "\n",
    "#choose memory driver\n",
    "memory_driver = gdal.GetDriverByName('GTiff')\n",
    "\n",
    "#create an empty raster and set the reference\n",
    "out_raster = memory_driver.Create(output_path, n_col, n_row, n_bands, gdal.gdal.GDT_Byte)\n",
    "out_raster.SetGeoTransform(geotransform)\n",
    "out_raster.SetProjection(projection)\n",
    "\n",
    "#write the classification array into the raster\n",
    "out_raster_ds.GetRasterBand(1).WriteArray(classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment - Classification playground:\n",
    "- Classify the image using at least two other algorithms.\n",
    "- You can use scikit-learn library (go through the documentation to find other methods) or some other you choose (for example XGBoost classifier has its own library).\n",
    "- Try to achieve the highest possible accuracy (using the same test set, generated by \"train_test_split(X, y, test_size=0.2, random_state=42)\").\n",
    "- Do not use the test set to tune hyperparameters! Only to evaluate the best model.\n",
    "- Save the classification, accuracy, confusion matrix and classification report.\n",
    "- Send a report to adam.kulich@natur.cuni.cz until XX.XX.2025 including:\n",
    "  - Best accuracy and confusion matrix.\n",
    "  - Detailed procedure used to achieve the result (including how you choose the methods you tried and why).\n",
    "  - All methods you tried and comparison of their best results.\n",
    "  - Resulting map of the land cover you made."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_39_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
